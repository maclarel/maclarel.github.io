<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=theme-color content="#494f5c"><meta name=msapplication-TileColor content="#494f5c"><meta itemprop=name content="Draining the (Big Data) Swamp"><meta itemprop=description content="Before we dive too far into this article, let&rsquo;s define a few key terms that will come up at several points:
 Big Data - Technology relating to the storage, management, and utilization of &ldquo;Big Data&rdquo; (e.g. enormous amounts of data/petabyte scale). Data Lake - A common term to refer to a storage platform for Big Data. In this case, let&rsquo;s assume Apache Hadoop (HDFS) or Amazon S3. Data Warehouse - A large store of data sourced from a variety of sources within a company, then used to guide management or business decisions."><meta itemprop=datePublished content="2018-07-24T00:00:00+00:00"><meta itemprop=dateModified content="2018-07-24T00:00:00+00:00"><meta itemprop=wordCount content="1665"><meta itemprop=keywords content="bigdata,bestpractices,"><meta property="og:title" content="Draining the (Big Data) Swamp"><meta property="og:description" content="Before we dive too far into this article, let&rsquo;s define a few key terms that will come up at several points:
 Big Data - Technology relating to the storage, management, and utilization of &ldquo;Big Data&rdquo; (e.g. enormous amounts of data/petabyte scale). Data Lake - A common term to refer to a storage platform for Big Data. In this case, let&rsquo;s assume Apache Hadoop (HDFS) or Amazon S3. Data Warehouse - A large store of data sourced from a variety of sources within a company, then used to guide management or business decisions."><meta property="og:type" content="article"><meta property="og:url" content="https://maclarel.github.io/posts/2018-07/draining-the-big-data-swamp/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2018-07-24T00:00:00+00:00"><meta property="article:modified_time" content="2018-07-24T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Draining the (Big Data) Swamp"><meta name=twitter:description content="Before we dive too far into this article, let&rsquo;s define a few key terms that will come up at several points:
 Big Data - Technology relating to the storage, management, and utilization of &ldquo;Big Data&rdquo; (e.g. enormous amounts of data/petabyte scale). Data Lake - A common term to refer to a storage platform for Big Data. In this case, let&rsquo;s assume Apache Hadoop (HDFS) or Amazon S3. Data Warehouse - A large store of data sourced from a variety of sources within a company, then used to guide management or business decisions."><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/safari-pinned-tab.svg color><link rel="shortcut icon" href=/favicon.ico><title>Draining the (Big Data) Swamp</title><link rel=stylesheet href=https://maclarel.github.io/css/style.min.037b6ee8f8c1baab6a3d0a9da11c3ff18a7552471f16c59fd98538d5ce99208b.css integrity="sha256-A3tu6PjBuqtqPQqdoRw/8Yp1UkcfFsWf2YU41c6ZIIs=" crossorigin=anonymous></head><body id=page><header id=site-header class="animated slideInUp"><div class="hdr-wrapper section-inner"><div class=hdr-left><div class=site-branding><a href=https://maclarel.github.io>Logan MacLaren</a></div><nav class="site-nav hide-in-mobile"><a href=https://maclarel.github.io/posts/>Blog</a></nav></div><div class="hdr-right hdr-icons"><button id=toc-btn class="hdr-btn desktop-only-ib" title="Table of Contents"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-list"><line x1="8" y1="6" x2="21" y2="6"/><line x1="8" y1="12" x2="21" y2="12"/><line x1="8" y1="18" x2="21" y2="18"/><line x1="3" y1="6" x2="3" y2="6"/><line x1="3" y1="12" x2="3" y2="12"/><line x1="3" y1="18" x2="3" y2="18"/></svg></button><span class="hdr-social hide-in-mobile"><a href=https://www.linkedin.com/in/loganmaclaren/ target=_blank rel="noopener me" title=Linkedin><svg xmlns="http://www.w3.org/2000/svg" class="feather" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg></a><a href=https://github.com/maclarel target=_blank rel="noopener me" title=Github><svg xmlns="http://www.w3.org/2000/svg" class="feather" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a><a href=https://infosec.exchange/@gill3tt3 target=_blank rel="noopener me" title=Twitter><svg xmlns="http://www.w3.org/2000/svg" class="feather" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg></a></span><button id=menu-btn class=hdr-btn title=Menu><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button></div></div></header><div id=mobile-menu class="animated fast"><ul><li><a href=https://maclarel.github.io/posts/>Blog</a></li></ul></div><main class="site-main section-inner animated fadeIn faster"><article class=thin><header class=post-header><div class=post-meta><span>Jul 24, 2018</span></div><h1>Draining the (Big Data) Swamp</h1></header><div class=content><p>Before we dive too far into this article, let&rsquo;s define a few key terms that will come up at several points:</p><ul><li><code>Big Data</code> - Technology relating to the storage, management, and utilization of &ldquo;Big Data&rdquo; (e.g. enormous amounts of data/petabyte scale).</li><li><code>Data Lake</code> - A common term to refer to a storage platform for Big Data. In this case, let&rsquo;s assume Apache Hadoop (HDFS) or Amazon S3.</li><li><code>Data Warehouse</code> - A large store of data sourced from a variety of sources within a company, then used to guide management or business decisions.</li><li><code>ETL</code> - Extract, Transform, Load. The process of taking data from one source, making changes to it, and loading it into another location. The underpinning of Master Data Management, and basically all data movement.</li><li><code>Master Data Management (MDM)</code> - The concept of merging important data into a single point of reference. For a lot of Big Data applications, this means tying data back to a single person or actor.</li></ul><h1 id=now-with-that-out-of-the-way-why-on-earth-is-the-name-of-this-article-talking-about-a-data-swamp>Now with that out of the way&mldr; why on earth is the name of this article talking about a Data Swamp?<a href=#now-with-that-out-of-the-way-why-on-earth-is-the-name-of-this-article-talking-about-a-data-swamp class=anchor aria-hidden=true><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 015 5 5 5 0 01-5 5h-3m-6 0H6a5 5 0 01-5-5 5 5 0 015-5h3"/><line x1="8" y1="12" x2="16" y2="12"/></svg></a></h1><p>Coming from the RDBMS/data warehouse world - data normalization and MDM is a critical part of any project involving data. Generally there will be some source system that is feeding data in that is then &ldquo;cleaned&rdquo; (or normalized) to reduce duplication/inconsistencies and is then fed into a data warehouse which is used for reporting.</p><p>Let&rsquo;s say that you&rsquo;re handling data coming in from two ubiquitous Microsoft platforms - Active Directory and Exchange. Both of these have relatively consistent logging formats to track access, or at least to track behaviour, but where things get different quickly is who they say is accessing things.</p><p>With Active Directory logs (let&rsquo;s just say evtx format for the sake of argument), you&rsquo;ll see the account name as <code>DOMAIN\User</code>, or in some cases <code>user@domain.com</code>. With Exchange, however, you&rsquo;re going to see data coming in under the user&rsquo;s email address or, if there&rsquo;s no authentication configured, whatever email address was specified as the sender&mldr; in some cases this is going to be <code>user@domain.com</code> however frequently it can also be <code>firstname.lastname@domain.com</code> or any other infinite number of ways that an organization chooses to reference email addresses.</p><p>To put this into a more real world example, at a company I used to work with my login for the domain was in the format of a country code and employee ID, e.g. <code>CA123456</code>, so AD would track me under that ID and only that ID. At that same company I had two email addresses - <code>tdn@company.com</code>, and <code>thedatanewbie@company.com</code> (short name and long name). See where this gets confusing!?</p><h1 id=ok-now-i-understand-that-data-is-a-mess-whats-your-point>OK, now I understand that data is a mess&mldr; What&rsquo;s your point?<a href=#ok-now-i-understand-that-data-is-a-mess-whats-your-point class=anchor aria-hidden=true><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 015 5 5 5 0 01-5 5h-3m-6 0H6a5 5 0 01-5-5 5 5 0 015-5h3"/><line x1="8" y1="12" x2="16" y2="12"/></svg></a></h1><p>Now picture yourself on the receiving end of those logs and trying to piece this all together into a working scenario where you can say that I logged in at 8AM at my normal office place and send out fifteen emails to thirty recipients&mldr; That&rsquo;s where MDM comes into play!</p><p>Despite the seemingly strange approach, there&rsquo;s usually a method to the madness in this sort of thing. Generally all of these items are referenced together in some place&mldr; generally an Active Directory user profile or some LDAP based equivalent. Easy to use, right!? Maybe&mldr;</p><p>Attributes in an LDAP profile can be public, meaning anyone who can do a lookup on the user can see them, or they can be private which means that you have to be authorized to see them. In a lot of cases the public attributes are only going to be group memberships, a real name, and the user name - this leaves a lot out to dry.</p><p>Let&rsquo;s assume, though, that we have privileged access and can now read all the attributes - we&rsquo;ve got our source of truth! Fantastic! Let&rsquo;s ignore the logistics of trying to maintain up to date LDAP lookups on demand for tens of thousands of users, let alone hundreds of thousands, and move on&mldr;</p><p>At this point a consulting team that gets paid lots of money is going to implement some sort of ETL pipeline that pulls data from the source system, does enrichment to tie this all to the &ldquo;one true version&rdquo; of a user, and then loads it into the Data Warehouse which will be used for reporting.</p><h1 id=this-all-sounds-great-weve-identified-how-to-enrich-the-data-ignoring-the-logistics-of-doing-so-and-now-youve-got-a-data-warehouse-you-can-report-off-of>This all sounds great! We&rsquo;ve identified how to enrich the data (ignoring the logistics of doing so), and now you&rsquo;ve got a Data Warehouse you can report off of!<a href=#this-all-sounds-great-weve-identified-how-to-enrich-the-data-ignoring-the-logistics-of-doing-so-and-now-youve-got-a-data-warehouse-you-can-report-off-of class=anchor aria-hidden=true><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 015 5 5 5 0 01-5 5h-3m-6 0H6a5 5 0 01-5-5 5 5 0 015-5h3"/><line x1="8" y1="12" x2="16" y2="12"/></svg></a></h1><p>Absolutely, it is great, but here is where we get back to the title of this article! Everything I just described is extremely common in the relational database world (e.g. Oracle, MS SQL Server, etc&mldr;) for reporting with products like Cognos, Qlik, etc&mldr; but unfortunately it is a concept that seems to be routinely ignored in the Big Data world.</p><p>As noted earlier, a lot of people will take these logs from many places and store them in their Data Lake for future use/analysis/etc&mldr; While it&rsquo;s great that they get stored, what this means is that you&rsquo;ve got likely petabytes worth of data that you&rsquo;re paying to store but are not using for anything and could not effectively use for anything that would not be extremely resource intensive to implement.</p><h1 id=this-is-why-big-data-implementations-fail>This is why Big Data implementations fail!<a href=#this-is-why-big-data-implementations-fail class=anchor aria-hidden=true><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 015 5 5 5 0 01-5 5h-3m-6 0H6a5 5 0 01-5-5 5 5 0 015-5h3"/><line x1="8" y1="12" x2="16" y2="12"/></svg></a></h1><p>Big Data is expensive, and its complex&mldr; it&rsquo;s the perfect storm of hard to do! With this said, all of the same concepts we talked about earlier still apply, they just have to be done differently! Running data through an ETL tool designed for an RDBMS that maybe operates on a total of 24 CPU cores and 128GB of RAM isn&rsquo;t going to cut it when you&rsquo;re dealing with enterprise levels of log files.</p><p>This is where I say it&rsquo;s expensive - not only do you need to have the expertise and knowledge of which tooling to use and how to use it, you need be willing to pay to make it happen. In a lot of cases for an enterprise handling authentication logs, email logs, endpoint logs, firewall logs, etc&mldr; this is going to be manifesting its self in the many dozens of CPU cores and hundreds of gigabytes of memory just to do the normalization. You can quickly see why there&rsquo;s hesitation to do this - who wants to spend upwards of half a million dollars on something that&rsquo;s just changing a user name in a log file!?</p><p>Unfortunately, without spending that money, those logs that are sitting in S3 (despite it being really cheap) are not really going to be useful if you want to use them down the road. This is where you wind up with the eponymous Data Swamp - a very murky lake full of data.</p><p>Many Data Lakes get set up as part of security initiatives so that there can be a postmortem on a breach, or even used proactively in the world of UEBA and Security Analytics, but without this MDM work (and in many cases even basic normalization) that company is going to wind up with the following challenges, to name a few:
Weeks or months of time spent identifying the characteristics and &ldquo;shapes&rdquo; of the data that is stored in the data lake.
The same cost that would have been incurred originally to now perform that MDM work (but under a time crunch).
Added costs and stress of trying to expedite an implementation of something to analyze the logs, or to have their security team(s) manually review them in a SIEM style tool like Splunk.
Really, it just circles back to what could have been done in the first place, and what would have allowed that company to have proactive tools in place, but even past that do immediate manual analysis or rules-based analysis of the data that was stored without having to incur all of the additional costs, stress, and time taken to do so after the fact.</p><h1 id=youre-painting-a-pretty-dire-picture-are-all-big-data-implementations-like-this>You&rsquo;re painting a pretty dire picture&mldr; Are all Big Data implementations like this?<a href=#youre-painting-a-pretty-dire-picture-are-all-big-data-implementations-like-this class=anchor aria-hidden=true><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 015 5 5 5 0 01-5 5h-3m-6 0H6a5 5 0 01-5-5 5 5 0 015-5h3"/><line x1="8" y1="12" x2="16" y2="12"/></svg></a></h1><p>NO! Thank goodness they are not, but the ones that fail all seem to have the same characteristics.</p><ul><li>No willingness to spend (usually this turns into all of this data living in S3 because it&rsquo;s cheap).</li><li>Limited forethought into the end use case (e.g. SIEM, Security Analytics, etc&mldr;).</li><li>No dedicated team for the platform, and thus no one to enable the end use case.</li></ul><p>The ones that are successful, however, tend to have done the following:</p><ul><li>Brought in, or hired, the expertise to handle the data movement and transformation, and continue to engage with those teams if they are not permanent.</li><li>Implement(ed) the Data Lake with a specific goal in mind, and identified the work required to make that happen.</li><li>Engaged with, or at least consulted, vendors that would enable the use case once the data was available.</li></ul><h1 id=lets-wrap-this-up>Let&rsquo;s wrap this up.<a href=#lets-wrap-this-up class=anchor aria-hidden=true><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 015 5 5 5 0 01-5 5h-3m-6 0H6a5 5 0 01-5-5 5 5 0 015-5h3"/><line x1="8" y1="12" x2="16" y2="12"/></svg></a></h1><p>Is this technically easy? Absolutely not. It&rsquo;s not easy even in the RDBMS world where a few dozen gigabytes of sales data across 500 retails stores is considered a huge volume of data to report off of. What I&rsquo;m saying at the end of this is that conceptually it&rsquo;s something that seems to get missed in a lot of Big Data implementations, and there are a lot of folks out there who have been blasting Big Data/Hadoop implementations for years despite the fact that a lot of those issues can be tied back to this at the end of the day.</p><p>If this struck a chord with you, and it&rsquo;s something you&rsquo;re having trouble with, keep an eye on this blog and the associated YouTube channel. We&rsquo;re going to be taking a look at a bunch of technology that enables this whole thing to work - Kafka, NiFi, Elasticsearch, Hadoop, etc&mldr; there&rsquo;s no shortage of platforms to let you do this work, and even choosing the &ldquo;wrong&rdquo; one generally won&rsquo;t hamstring you too much, but you have to make that leap to get started!</p></div><hr class=post-end><footer class=post-info><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z"/><line x1="7" y1="7" x2="7" y2="7"/></svg><span class=tag><a href=https://maclarel.github.io/tags/bigdata>bigdata</a></span><span class=tag><a href=https://maclarel.github.io/tags/bestpractices>bestpractices</a></span></p><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="16" y1="13" x2="8" y2="13"/><line x1="16" y1="17" x2="8" y2="17"/><polyline points="10 9 9 9 8 9"/></svg>1665 Words</p><p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>2018-07-24 00:00 +0000</p></footer></article><aside id=toc><div class=toc-title>Table of Contents</div><nav id=TableOfContents></nav></aside><div class="post-nav thin"><a class=next-post href=https://maclarel.github.io/posts/2020-04/starting-out-with-a-home-lab/><span class=post-nav-label><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-left"><line x1="19" y1="12" x2="5" y2="12"/><polyline points="12 19 5 12 12 5"/></svg>&nbsp;Newer</span><br><span>Starting out with a home lab</span></a>
<a class=prev-post href=https://maclarel.github.io/posts/2018-04/self-reliant/><span class=post-nav-label>Older&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-right"><line x1="5" y1="12" x2="19" y2="12"/><polyline points="12 5 19 12 12 19"/></svg></span><br><span>Being Self-Reliant with Technology</span></a></div><div id=comments class=thin></div></main><footer id=site-footer class="section-inner thin animated fadeIn faster"><p>&copy; 2022 <a href=https://maclarel.github.io>Logan MacLaren</a> &#183; <a href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank rel=noopener>CC BY-NC 4.0</a> &#183; <a rel=me href=https://infosec.exchange/@gill3tt3>Mastodon</a></p><p>Made with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> &#183; Theme <a href=https://github.com/Track3/hermit target=_blank rel=noopener>Hermit</a> &#183; <a href=https://maclarel.github.io/posts/index.xml target=_blank title=rss><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></p></footer><script src=https://maclarel.github.io/js/bundle.min.7d8545daa55d62427355498dd8da13f98ff79a7938ce7d2a5e2ae1ec0de3beb8.js integrity="sha256-fYVF2qVdYkJzVUmN2NoT+Y/3mnk4zn0qXirh7A3jvrg=" crossorigin=anonymous></script></body></html>